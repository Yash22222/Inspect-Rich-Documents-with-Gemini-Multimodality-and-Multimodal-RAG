# Multimodality with Gemini

## 🔍 Overview

In this lab, you'll explore how to use **Gemini’s multimodal capabilities** within **Vertex AI** to process and analyze **visual and textual inputs together**. This is useful for applications where both images and text are needed to derive meaningful insights.

## 🎯 Objectives

- Initialize and use **Vertex AI** in your environment.
- Load the **Gemini multimodal model** using Python SDK.
- Send a **combination of image and text** as input.
- Generate and interpret **AI-driven responses** based on the multimodal input.

## 🛠️ Tasks

1. **Set up Vertex AI** in your Google Cloud project.
2. **Load Gemini multimodal model** (`gemini-1.0-pro-vision`).
3. Use an example image and question like:
   - _Image_: A food item (e.g., scones)
   - _Prompt_: "What is shown in this image?"
4. **Receive and display AI-generated answers.**

## 📚 Learning Outcome

By completing this lab, you’ll understand how to:

- Build AI systems that **combine image + text inputs**.
- Use **multimodal models** for smarter, context-aware responses.
- Prepare the foundation for advanced **GenAI applications** like document analysis, product recognition, etc.

## ✅ Prerequisites

- Access to **Google Cloud Vertex AI**.
- Basic familiarity with **Python** and **Google Cloud SDK**.

---
